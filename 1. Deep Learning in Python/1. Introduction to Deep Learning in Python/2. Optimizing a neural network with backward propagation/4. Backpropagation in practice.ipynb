{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T11:20:21.989524Z",
     "start_time": "2020-07-24T11:20:12.960881Z"
    }
   },
   "source": [
    "```\n",
    "###############################\n",
    "##                           ##\n",
    "##  Deep Learning in Python  ##\n",
    "##                           ##\n",
    "###############################\n",
    "\n",
    "ยง1 Introduction to Deep Learning in Python\n",
    "\n",
    "ยง1.2 Optimizing a neural network with backward propagation\n",
    "\n",
    "ยง1.2.4 Backpropagation in practice\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. How to calculate slopes associated with any weight?**\n",
    "\n",
    "* Gradients for weight is the product of:\n",
    "\n",
    "    * 1. node value feeding into that weight\n",
    "    \n",
    "    * 2. slope of activation function for the node being fed into\n",
    "    \n",
    "    * 3. slope of the loss function with respect to the output node\n",
    "\n",
    "![Calculating slopes associated with any weight](ref9.%20Calculating%20slopes%20associated%20with%20any%20weight.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. To recap backpropagation, what are the steps?**\n",
    "\n",
    "* Start at some random set of weights.\n",
    "\n",
    "* Use forward propagation to make a prediction.\n",
    "\n",
    "* Use backward propagation to calculate the slope of the loss function with respect to each weight.\n",
    "\n",
    "* Multiply that slope by the learning rate, and subtract from the current weights.\n",
    "\n",
    "* Keep going with that cycle until got a flat part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What are the principles of stochastic gradient descent?**\n",
    "\n",
    "* What are the principles of stochastic gradient descent?\n",
    "\n",
    "* It is common to calculate slopes on only a subset of the data (a batch).\n",
    "\n",
    "* Use a different batch of data to calculate the next update.\n",
    "\n",
    "* Start over from the beginning once all data is used.\n",
    "\n",
    "* Each time through the training data is called an epoch.\n",
    "\n",
    "* When slopes are calculated on one batch at a time, it is so-called stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Practice question for a round of backpropagation:**\n",
    "\n",
    "* In the network shown below, forward propagation was done, and node values calculated as part of the forward propagation are shown in white. The weights are shown in black. Layers after the question mark show the slopes calculated as part of back-prop, rather than the forward-prop values. Those slope values are shown in purple.\n",
    "\n",
    "* This network again uses the ReLU activation function, so the slope of the activation function is $1$ for any node receiving a positive value as input. Assume the node being examined had a positive value (so the activation function's slope is $1$).\n",
    "    \n",
    "    ![A round of backpropagation_1](ref10.%20A%20round%20of%20backpropagation_1.png)\n",
    "\n",
    "* What is the slope needed to update the weight with the question mark?\n",
    "\n",
    "    ![A round of backpropagation_2](ref11.%20A%20round%20of%20backpropagation_2.png)\n",
    "\n",
    "    $\\Box$ $0$.\n",
    "    \n",
    "    $\\Box$ $2$.\n",
    "    \n",
    "    $\\boxtimes$ $6$.\n",
    "\n",
    "    $\\Box$ Not enough information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": true
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
